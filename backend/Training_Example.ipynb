{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "\n",
    "d_model = 512\n",
    "batch_size = 2\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "max_sequence_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flutter Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 126\n"
     ]
    }
   ],
   "source": [
    "from pygments.lexers import DartLexer\n",
    "from utils.vocab import dart_vocab\n",
    "from utils.code_tokenizer import CodeTokenizer\n",
    "\n",
    "flutter_tokenizer = CodeTokenizer(\n",
    "    DartLexer(),\n",
    "    framework_vocab=[\"Scaffold\", \"Widget\", \"setState\"],\n",
    "    language_vocab=dart_vocab,\n",
    "    START_TOKEN=START_TOKEN,\n",
    "    END_TOKEN=END_TOKEN,\n",
    "    PAD_TOKEN=PADDING_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"Token Count: {len(flutter_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create React Native Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pygments.lexers import JavascriptLexer\n",
    "from utils.vocab import javascript_vocab\n",
    "from utils.code_tokenizer import CodeTokenizer\n",
    "\n",
    "react_native_tokenizer = CodeTokenizer(\n",
    "    JavascriptLexer(),\n",
    "    framework_vocab=[\"View\", \"Text\", \"useState\"],\n",
    "    language_vocab=javascript_vocab,\n",
    "    START_TOKEN=START_TOKEN,\n",
    "    END_TOKEN=END_TOKEN,\n",
    "    PAD_TOKEN=PADDING_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"Token Count: {len(react_native_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Letter Tokenizer\n",
    "For debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 101\n"
     ]
    }
   ],
   "source": [
    "from utils.code_tokenizer import CodeTokenizer\n",
    "\n",
    "letter_tokenizer = CodeTokenizer(\n",
    "    None,\n",
    "    framework_vocab=[],\n",
    "    language_vocab=[\n",
    "        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "        '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '{', '}', '(', ')',\n",
    "        '[', ']', '=', '+', '-', '*', '/', '%', '^', '&', '|', '!', '?', '<',\n",
    "        '>', ':', ';', ',', '.', '_', '#', '@', '$', '~', '`', '\"', \"'\", '\\\\',\n",
    "        '/', '\\n', ' ', '\\t', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',\n",
    "        'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "        'W', 'X', 'Y', 'Z'\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Token Count: {len(letter_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SnippetEmbedding(\n",
       "      (embedding): Embedding(126, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SnippetEmbedding(\n",
       "      (embedding): Embedding(128, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNormalization()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    d_model, \n",
    "    ffn_hidden,\n",
    "    num_heads, \n",
    "    drop_prob, \n",
    "    num_layers, \n",
    "    max_sequence_length,\n",
    "    flutter_tokenizer,\n",
    "    react_native_tokenizer,\n",
    ")\n",
    "\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, a_snippets, a_parser, b_snippets, b_parser):\n",
    "        self.a_snippets = a_snippets\n",
    "        self.a_parser = a_parser\n",
    "        self.b_snippets = b_snippets\n",
    "        self.b_parser = b_parser\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.a_snippets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.a_snippets[idx], self.b_snippets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Dart samples: 212\n",
      "Number of JS samples: 212\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.parsing.code_parser import DartParser, JavascriptParser\n",
    "\n",
    "df = pd.read_csv('./data/raw/samples.csv')\n",
    "\n",
    "dart_samples = df['dart'].values\n",
    "js_samples = df['javascript'].values\n",
    "\n",
    "# filter for max length of sample at `max_sequence_len`` characters for training purposes\n",
    "zipped = list(zip(dart_samples, js_samples))\n",
    "samples = [(d, j) for d, j in zipped]\n",
    "dart_samples, js_samples = zip(*samples)\n",
    "\n",
    "print(\"Number of Dart samples:\", len(dart_samples))\n",
    "print(\"Number of JS samples:\", len(js_samples))\n",
    "\n",
    "dataset = TextDataset(dart_samples, DartParser(\"\"), js_samples, JavascriptParser(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=react_native_tokenizer[PADDING_TOKEN], reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.AdamW(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "NEG_INFTY = -1e9\n",
    "\n",
    "def bracket_pairs_indices(tokens):\n",
    "    stacks = defaultdict(list)\n",
    "    pairs = {}\n",
    "    for i, (_, token) in enumerate(tokens):\n",
    "        if token == '(':\n",
    "            stacks['PAREN'].append(i)\n",
    "        elif token == ')':\n",
    "            if len(stacks['PAREN']) > 0:\n",
    "                pairs[stacks['PAREN'].pop()] = i\n",
    "        if token == '[':\n",
    "            stacks['BRACKET'].append(i)\n",
    "        elif token == ']':\n",
    "            if len(stacks['BRACKET']) > 0:\n",
    "                pairs[stacks['BRACKET'].pop()] = i\n",
    "        if token == '{':\n",
    "            stacks['BRACE'].append(i)\n",
    "        elif token == '}':\n",
    "            if len(stacks['BRACE']) > 0:\n",
    "                pairs[stacks['BRACE'].pop()] = i\n",
    "    return pairs\n",
    "\n",
    "def create_masks(flutter_batch, react_batch):\n",
    "    num_seqs = len(flutter_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "\n",
    "    encoder_padding_mask = torch.full([num_seqs, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_seqs, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_seqs, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_seqs):\n",
    "        flutter_sequence, react_sequence = flutter_tokenizer.tokenize(flutter_batch[idx]), react_native_tokenizer.tokenize(react_batch[idx])\n",
    "        flutter_seq_length, react_seq_length = len(flutter_sequence), len(react_sequence)\n",
    "        \n",
    "        flutter_tokens_to_padding_mask = np.arange(flutter_seq_length + 1, max_sequence_length)\n",
    "        react_tokens_to_padding_mask = np.arange(react_seq_length + 1, max_sequence_length)\n",
    "        \n",
    "        encoder_padding_mask[idx, :, flutter_tokens_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, flutter_tokens_to_padding_mask, :] = True\n",
    "        \n",
    "        decoder_padding_mask_self_attention[idx, :, react_tokens_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, react_tokens_to_padding_mask, :] = True\n",
    "        \n",
    "        decoder_padding_mask_cross_attention[idx, :, flutter_tokens_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, react_tokens_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(look_ahead_mask + encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:22<00:00,  6.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        flutter_batch, react_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(flutter_batch, react_batch)\n",
    "        optim.zero_grad()\n",
    "        react_pred = transformer(\n",
    "            flutter_batch,\n",
    "            react_batch,\n",
    "            encoder_self_attention_mask.to(device), \n",
    "            decoder_self_attention_mask.to(device), \n",
    "            decoder_cross_attention_mask.to(device),\n",
    "        )\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(react_batch)\n",
    "        loss = criterian(\n",
    "            react_pred.view(-1, len(react_native_tokenizer)).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == react_native_tokenizer[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flutter code:   bool isPalindrome(int x) {\n",
      "    if (x < 0 || (x % 10 == 0 && x != 0)) return false;\n",
      "  \n",
      "    int revertedNumber = 0;\n",
      "    while (x > revertedNumber) {\n",
      "      revertedNumber = revertedNumber * 10 + x % 10;\n",
      "      x ~/= 10;\n",
      "    }\n",
      "  \n",
      "    return x == revertedNumber || x == revertedNumber ~/ 10;\n",
      "  }\n",
      "  \n",
      "Tokenized Flutter Code: [(1, '<START>'), (4, ' '), (4, ' '), (4, 'bool'), (4, ' '), (3, 'UNK'), (73, '('), (4, 'int'), (4, ' '), (3, 'UNK'), (74, ')'), (4, ' '), (112, '{'), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (34, 'if'), (4, ' '), (73, '('), (3, 'UNK'), (4, ' '), (92, '<'), (4, ' '), (3, 'UNK'), (4, ' '), (113, '|'), (113, '|'), (4, ' '), (73, '('), (3, 'UNK'), (4, ' '), (68, '%'), (4, ' '), (3, 'UNK'), (4, ' '), (96, '='), (96, '='), (4, ' '), (3, 'UNK'), (4, ' '), (70, '&'), (70, '&'), (4, ' '), (3, 'UNK'), (4, ' '), (66, '!'), (96, '='), (4, ' '), (3, 'UNK'), (74, ')'), (74, ')'), (4, ' '), (49, 'return'), (4, ' '), (27, 'false'), (91, ';'), (121, '\\n'), (4, ' '), (4, ' '), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (4, 'int'), (4, ' '), (3, 'UNK'), (4, ' '), (96, '='), (4, ' '), (3, 'UNK'), (91, ';'), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (63, 'while'), (4, ' '), (73, '('), (3, 'UNK'), (4, ' '), (98, '>'), (4, ' '), (3, 'UNK'), (74, ')'), (4, ' '), (112, '{'), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (3, 'UNK'), (4, ' '), (96, '='), (4, ' '), (3, 'UNK'), (4, ' '), (75, '*'), (4, ' '), (3, 'UNK'), (4, ' '), (77, '+'), (4, ' '), (3, 'UNK'), (4, ' '), (68, '%'), (4, ' '), (3, 'UNK'), (91, ';'), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (3, 'UNK'), (4, ' '), (117, '~'), (88, '/'), (96, '='), (4, ' '), (3, 'UNK'), (91, ';'), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (116, '}'), (121, '\\n'), (4, ' '), (4, ' '), (121, '\\n'), (4, ' '), (4, ' '), (4, ' '), (4, ' '), (49, 'return'), (4, ' '), (3, 'UNK'), (4, ' '), (96, '='), (96, '='), (4, ' '), (3, 'UNK'), (4, ' '), (113, '|'), (113, '|'), (4, ' '), (3, 'UNK'), (4, ' '), (96, '='), (96, '='), (4, ' '), (3, 'UNK'), (4, ' '), (117, '~'), (88, '/'), (4, ' '), (3, 'UNK'), (91, ';'), (121, '\\n'), (4, ' '), (4, ' '), (116, '}'), (121, '\\n'), (4, ' '), (4, ' '), (121, '\\n'), (2, '<END>')]\n",
      "React Translation: <START>abstractabstractabstract<UNK><<UNK>>=<UNK>(<<UNK>>(null<UNK>{(null<<UNK>>{(null%=<UNK>%={(null=><<UNK>>%={(null=>??<UNK>const=>{(null=>&<<UNK>>const=>{(null=>&%<UNK>=>const=>{(null=>const%<<UNK>>=>const=>{(null=>const%...<UNK>delete>const=>{(null=>const%...<<UNK>>~>const=>{(null=>const%...%<UNK>>>>~>const=>{(null=>const%in%<<UNK>==>|>const=>{(null=>const%*%<<<UNK>>==>|>const=>{(null=>const%*%<<<<UNK>===>==>const=>{(null=>const%*%<<<<<UNK>>===>==>const=>{(null=>const%*%nullnull<<UNK>!=!=>==>const=>{(null=>const%*%<UNK>%<UNK>>constconst>==>const=>{(null=>const%*%<<UNK>>%<<UNK>--<UNK>>==>const=>{(null=>const%*%--<UNK>false%<<<UNK>>--<<UNK>true=>const=>{(null=>const%*%--<<UNK>>false%%<<UNK>break--%<UNK>>true=>const=>{(null=>const%*%--continue<UNK>-=false%%-=<UNK>>-=--%<<UNK>truetrue=>const=>{(nulltrueconst%*%--continue<<UNK>>truefalse%%-=<<UNK>casetrue--%%<UNK>><UNK>=>const=>{(<UNK>%*%--continue<<<UNK>-=<UNK>%%-=%<UNK>><UNK>--%%<<UNK>-=<<UNK>>=>const=>{(<<UNK>>%*%--continue--<<UNK>>-=<<UNK>>%%true%<<UNK>true<<UNK>>--%%true<UNK>>truetrue<UNK>break>const=>{(case<UNK><END>\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "# predict the translation of a flutter code snippet\n",
    "flutter_code = dart_samples[8]\n",
    "print(f\"Flutter code: {flutter_code}\")\n",
    "\n",
    "tokenized_flutter_code = flutter_tokenizer.tokenize(flutter_code)\n",
    "print(f\"Tokenized Flutter Code: {tokenized_flutter_code}\")\n",
    "\n",
    "flutter_code = (flutter_code,)\n",
    "react_code = (\"\",)\n",
    "for word_counter in range(max_sequence_length):\n",
    "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(flutter_code, react_code)\n",
    "    predictions = transformer(\n",
    "        flutter_code,\n",
    "        react_code,\n",
    "        encoder_self_attention_mask.to(device), \n",
    "        decoder_self_attention_mask.to(device), \n",
    "        decoder_cross_attention_mask.to(device),\n",
    "    )\n",
    "    next_token_prob_distribution = predictions[0][word_counter]\n",
    "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "    next_token = react_native_tokenizer.get_token(next_token_index)\n",
    "    react_code = (react_code[0] + next_token, )\n",
    "    if next_token == END_TOKEN:\n",
    "        break\n",
    "print(f\"React Translation: {react_code[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(transformer.state_dict(), './models/transformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
